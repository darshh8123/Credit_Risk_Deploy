{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2494,
     "status": "ok",
     "timestamp": 1757261883189,
     "user": {
      "displayName": "Darshan Darsh",
      "userId": "13382084090601068611"
     },
     "user_tz": -330
    },
    "id": "VoCx-QUO3Taj",
    "outputId": "3d22ffae-4e66-4c69-e683-8aad16dc5558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully from URL.\n",
      "\n",
      "Training the Random Forest model for Credit Risk Prediction...\n",
      "Training complete.\n",
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.84       140\n",
      "           1       0.65      0.37      0.47        60\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.71      0.64      0.65       200\n",
      "weighted avg       0.73      0.75      0.73       200\n",
      "\n",
      "\n",
      "New model saved to 'credit_risk_pipeline.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# --- Step 1: Automatically Download the German Credit Dataset ---\n",
    "# This code downloads the dataset directly from a public URL.\n",
    "# This avoids any FileNotFoundError issues.\n",
    "try:\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
    "\n",
    "    # The dataset has no header row, so we define the column names manually.\n",
    "    # The last column (20th) is the target variable.\n",
    "    column_names = ['checking_account', 'duration', 'credit_history', 'purpose', 'credit_amount',\n",
    "                    'savings_account', 'employment_since', 'installment_rate', 'personal_status',\n",
    "                    'other_debtors', 'residence_since', 'property', 'age', 'other_installment_plans',\n",
    "                    'housing', 'existing_credits', 'job', 'dependents', 'telephone', 'foreign_worker',\n",
    "                    'credit_risk']\n",
    "\n",
    "    df = pd.read_csv(url, sep=' ', header=None, names=column_names)\n",
    "    print(\"Dataset loaded successfully from URL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while downloading the dataset: {e}\")\n",
    "    print(\"Please check your internet connection or the URL.\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 2: Feature Engineering and Data Preparation ---\n",
    "# The target variable (credit_risk) is currently 1 for good and 2 for bad.\n",
    "# We convert it to 0 for good and 1 for bad for machine learning.\n",
    "df['credit_risk'] = df['credit_risk'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Define the features and the target variable\n",
    "numerical_features = ['duration', 'credit_amount', 'installment_rate', 'age', 'existing_credits', 'dependents']\n",
    "categorical_features = ['checking_account', 'credit_history', 'purpose', 'savings_account',\n",
    "                        'employment_since', 'personal_status', 'other_debtors',\n",
    "                        'property', 'other_installment_plans', 'housing',\n",
    "                        'job', 'telephone', 'foreign_worker']\n",
    "target = 'credit_risk'\n",
    "\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[target]\n",
    "\n",
    "# --- Step 3: Create the Preprocessing Pipeline ---\n",
    "# This pipeline scales numerical data and one-hot encodes categorical data.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
    "\n",
    "# --- Step 4: Create and Train the Model Pipeline ---\n",
    "# Using a RandomForestClassifier with balanced class weights.\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=500, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining the Random Forest model for Credit Risk Prediction...\")\n",
    "\n",
    "# Split the data and train the pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Step 5: Evaluate the Model ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Step 6: Save the Trained Pipeline ---\n",
    "joblib.dump(pipeline, 'credit_risk_pipeline.pkl')\n",
    "print(\"\\nNew model saved to 'credit_risk_pipeline.pkl'.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
